{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. K-means\n",
    "- Find K-clusters from 'unlabeled' data\n",
    "\n",
    "- Applications:\n",
    "    - Segment user purchasing behavior\n",
    "    - Cluster plhysicians together\n",
    "    \n",
    "    \n",
    "- Working:\n",
    "    - Initialize cluster centroids, different initilizations will lead to different results\n",
    "    - Assign each data point to its closest centroid, and then calculate the mean for each cluster, and then update the centroid with the mean\n",
    "    - This process is repeated until convergence i.e. clusters/centroids and label assignment stablizes\n",
    "    - If the movement of the centroids is smaller than the set threshold, we get out of loop \n",
    "    \n",
    "    \n",
    "- Complexity:\n",
    "    - O(K *N *I) Time\n",
    "    - O(N) Space\n",
    "    - K: clusters, N-data points, I-iterations\n",
    "  \n",
    "- Functions:\n",
    "    - Initialize centroids\n",
    "    - Get labels using distance\n",
    "    - Update centroids\n",
    "    - Stopping function\n",
    "    - Runner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(data, k):\n",
    "    import random\n",
    "    \n",
    "    ## Initialize the k-centroids in the data space. We are considering a 2-D data here\n",
    "    centroids = initialize_centroids(data, k)\n",
    "    \n",
    "    while True:\n",
    "        old_centroids = centroids\n",
    "        ## Find the label for each data-point \n",
    "        labels = get_labels(data, centroids)\n",
    "        ## Find the new centroids based on the labels detected for each data point, keep updating the centroids and labels until convergence\n",
    "        centroids = update_centroids(data, labels, k)\n",
    "        \n",
    "        if should_stop(old_centroids, centroids):\n",
    "            break\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  Initialize centroids\n",
    "### Idea is to randomly initialize the centroids within the data range\n",
    "#\n",
    "\n",
    "def initialize_centroids(data, k):\n",
    "        ## Starting with -inf to inf space\n",
    "        ## The reason we choose inf as minimum is because upon comparing it with the data, the data-point is always gonna be smaller than inf\n",
    "        x_min = y_min = float('inf')\n",
    "        x_max = y_max = float('-inf')\n",
    "        \n",
    "        ## let's iterate through the data to find the range of the data\n",
    "        for point in data:\n",
    "            x_min = min(point[0], x_min)\n",
    "            x_max = max(point[0], x_max)\n",
    "            y_min = min(point[1], y_min)\n",
    "            y_max = max(point[1], y_max)\n",
    "        \n",
    "        centroids = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            centroids.append([random_sample(x_min, x_max), random_sample(y_min, y_max)])\n",
    "        \n",
    "        return centroids\n",
    "\n",
    "### Returns a number uniformly distributed between 0 and 1\n",
    "def random_sample(low, high):\n",
    "    return low+(high-low)*random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get labels for the data points\n",
    "##\n",
    "#\n",
    "\n",
    "def get_labels(data, centroids):\n",
    "    labels = []\n",
    "    for point in data:\n",
    "        min_dist = float('inf')\n",
    "        label = None\n",
    "        \n",
    "        for i, centroid in enumerate(centroids):\n",
    "            new_dist = get_distance(point, centroid)\n",
    "            if new_dist<min_dist:\n",
    "                min_dist = new_dist\n",
    "                label = i\n",
    "                \n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "\n",
    "### Get distance between two points\n",
    "## \n",
    "\n",
    "def get_distance(point_A, point_B):\n",
    "    return ((point_B[1]-point_A[1])**2 + (point_B[0]-point_A[0])**2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update the centroids\n",
    "##\n",
    "def update_centroids(data, labels, k):\n",
    "    ### Initializing new centroids at the origin   \n",
    "    new_centroids = [[0,0] for i in range(0,k)]\n",
    "    counts = [0]*k\n",
    "    \n",
    "    ## Iterating through the data points and their respective labels\n",
    "    for point, label in zip(data, label):\n",
    "        new_centroids[label][0]+=point[0]\n",
    "        new_centroids[label][1]+=point[1]\n",
    "        counts[label]+=1\n",
    "        \n",
    "    for i, (x,y) in enumerate(new_centroids):\n",
    "        new_centroids[i] = (x/counts[i], y/counts[i])\n",
    "        \n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### stopping criteria\n",
    "##\n",
    "\n",
    "def should_stop(old_centroids, new_centroids, threshold = 1e-5):\n",
    "    ## Using a very small threshold to stop the loop\n",
    "    total_movement = 0\n",
    "    \n",
    "    for old_point, new_point in zip(old_centroids, new_centroids):\n",
    "        total_movement+=get_distance(old_point, new_point)\n",
    "    return total_movement<threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 2. K-nearest neighbors\n",
    "    - You're determined by your closest neighbors\n",
    "    - Doesn't need to lean parameters like LR or logistic regression, can be used for classification/regression\n",
    "    - Prediction is made based on finding K-closest neighbors (existing data points)\n",
    "    - Distances used are eucleadian distance/cosine similarity\n",
    "    - For Regression, you take the average of all the neighbor values, that is now the target value of the new data point\n",
    "    - For Classification, you predict a new data point as the majority of the neighbor's class\n",
    "\n",
    "- Application:\n",
    "    - Finding the price of a new apartment\n",
    "    \n",
    "    \n",
    "- Implementation\n",
    "    - Obtaining the data\n",
    "    - Querying the nearest neighbors for prediction\n",
    "    \n",
    "    \n",
    "- Complexity:\n",
    "    - O(M log(M)) Time\n",
    "    - O(M) Space\n",
    "    - M: number of features\n",
    "\n",
    "- How to choose K:\n",
    "    - Pre-determined (arbitrary)\n",
    "    - simple approach K = (# data points)**0.5\n",
    "    - Cross validation (Use training data to test hyperparameters)\n",
    "        - Partition the data into n-parts (say 10), pick a range of values for hyper-parameter k - range(1, max(k)) e.g. [1,4,7]\n",
    "        - For each K, select a validation set and compute the validation error (MSE, or classification error) [**Remember there is no training involved in K-nearest neighbors]\n",
    "        - The K with smallest validation error on a validation set is your optimal K\n",
    "        - Another robust approach is to select each n-partition for each K, and calculate the validation error. For 10 parts, for each K, we will have 10 validation errors, take their mean for each K, the one with smallest mean-validation error will be your optimal K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "\n",
    "- Class KNN\n",
    "- train\n",
    "- predict\n",
    "- distance\n",
    "- classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### X = [ [X_00, X_0n], \n",
    "#         [X_m0, Xmn] ] where X is a 2-D array, where rows is data-points and columns are features\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def distance(self,point_A, point_B):\n",
    "        return ((point_B[1]-point_A[1])**2 + (point_B[0]-point_A[0])**2)**0.5\n",
    "    \n",
    "    def classification(self, neighbors):\n",
    "        ## For classification we need to find the most common label\n",
    "        neighbor_labels = [label for dist, label in neighbors] ## Storing the labels for all the neighbors\n",
    "        \n",
    "        ## Finding the most common label\n",
    "        labelDict={}\n",
    "        max_val = -999\n",
    "        \n",
    "        for label in neighbor_labels:\n",
    "            if label in labelDict:\n",
    "                labelDict[label]+=1\n",
    "            else:\n",
    "                labelDict[label]=1\n",
    "            if labelDict[label]>max_val:\n",
    "                max_val = labelDict[label]\n",
    "                \n",
    "        return max_val\n",
    "        \n",
    "    ## Predict the value for a new data-point using k-neighbors    \n",
    "    def predict(self, x, k, problem):\n",
    "        \n",
    "        ## Find the distance of the new data-point with all the data points, and store this distance as a tuple along with the label\n",
    "        distance_label = [(self.distance(x, train_point), train_label) for train_point, train_label in zip(self.x, self.y)]       \n",
    "        \n",
    "        ### Sort the distances in ascending order for the k neighbors\n",
    "        neighbors  =sorted(distance_label)[:k]\n",
    "        \n",
    "        if problem==reg:\n",
    "            ## Do the average and return the label/y-value for the new data point\n",
    "            return sum(label for _, label in neighbors)/k\n",
    "        else:\n",
    "            return self.classification(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 3. Linear Regression\n",
    "\n",
    "- Basics\n",
    "    - Fitting a straight line through a set of data points\n",
    "    - Relation between x and y is linear (This is an important assumption) WHY?\n",
    "    - MSE is used in LR, square of the difference b/w observed and actual value of y, divided by the total number of observations\n",
    "        - Goal is to get betas to minimize this error, hence, it is an optimization problem\n",
    "    - OLS: Ordinary Least Squares is an algorithm to estimate betas to get the least MSE for a data using a line.\n",
    "    \n",
    "    - The fitted line can calculate R2 and determine if two variables are correlated. Large values imply large effect\n",
    "    - Calculate p-value to determine if the R2 is statistically significant\n",
    "    - And of course, use this line to predict y from x\n",
    "\n",
    "- Equation\n",
    "    - yHat = bNot + b1*x\n",
    "    - x is the independent variable, yHat is the dependent variable/predicted value\n",
    "    - b1 is the slope of the line, and bNot is the constant/intercept in the straight line equation y=mx+c\n",
    "\n",
    "\n",
    "- Gradient Descent\n",
    "    - Start with Random guess of betas\n",
    "    - Compute MSE\n",
    "    - Compute gradients (derivative of error w.r.t. particular parameters) and update betas\n",
    "    - Repeat until convergence, i.e. the error reaches a local minima\n",
    "    - Learning rate determines the speed of convergence or movement of gradient descent. Setting it too high will make it unstable, and setting it too low will make it really slow to converge\n",
    "    - Partial derivative means, if we change the input by a small value say 0.01 then how does the output change. How does loss change with weights/betas\n",
    "\n",
    "\n",
    "- Application\n",
    "    - Predict demand with Price  \n",
    "    \n",
    "- Complexity:\n",
    "    - O(I *M *N) Time\n",
    "    - O(N) Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions\n",
    "- Main\n",
    "- initialize params\n",
    "- compute gradient\n",
    "- update betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Main function that will call helper functions\n",
    "###\n",
    "##\n",
    "\n",
    "import random\n",
    "\n",
    "def linear_regression(x, y, iterations=100, learning_rate=0.01):\n",
    "    ## n is the total number of features (means n+1 betas) and m is the total # of records\n",
    "    n, m = len(x[0]), len(x)\n",
    "    ## Initializing the # parameters based on number of features\n",
    "    beta_0, beta_other = initialize_params(n)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        gradient_beta_0, gradient_beta_other = compute_gradient(x, y, beta_0, beta_other, n, m)\n",
    "        beta_0, beta_other = update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate)\n",
    "        \n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameter initialization\n",
    "##  \n",
    "def initialize_params(dimensions):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for _ in range(dimensions)]\n",
    "    return beta_0, beta_other\n",
    "\n",
    "\n",
    "#### Computing the gradient\n",
    "##  \n",
    "def compute_gradient(x, y, beta_0, beta_other, dimension, records):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0]*dimension #dimension is the total number of features/columns in the data\n",
    "    m = records\n",
    "    \n",
    "    ## Iterating through each record and calculating the predicted value based on the current betas, multiplying it with the corresponding features and then\n",
    "    # adding beta_0 to it\n",
    "    for i in range(m):\n",
    "        ## Calculating prediction for each data point i\n",
    "        y_i_hat = sum(x[i][j] * beta_other[j] for j in range(dimension)) + beta_0\n",
    "        ## Calculating the derivative of error\n",
    "        derror_dy = 2*(y[i]-y_i_hat)  ## WHY error x 2\n",
    "        \n",
    "        for j in range(dimension):\n",
    "            ## Diving the gradients by m, so that at the end the gradient computed is the average of all data points\n",
    "            ## Following is the gradient formulae that gives us the partial derivative of loss function w.r.t. each parameter\n",
    "            gradient_beta_other[j] += derror_dy*x[i][j]/m\n",
    "            gradient_beta_0 += derror_dy/m\n",
    "        \n",
    "    return gradient_beta_0, gradient_beta_other\n",
    "\n",
    "#### Update params\n",
    "##  \n",
    "def update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate):\n",
    "    beta_0 += gradient_beta_0*learning_rate\n",
    "    for i in range(0, len(beta_other)):\n",
    "        beta_other[i] += gradient_beta_other[i]*learning_rate  ## Why there is a +ve sign here\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 4. Logistic Regression\n",
    "\n",
    "- Basics\n",
    "    - Used for binary classification problems\n",
    "    - All it does is take linear combination of all the features (log odds), and pass it through a sigmoid function to calculate the probability of a data point of being in class 1\n",
    "    - 1-prob(class-1) is the probability of class 0\n",
    "    - f(z) = 1/(1+e**-z), larger numbers get probability close to 1, and small numbers get probability close to 0. Based on a threshold, we decide whether a data point should be classfied to be in class 0 or 1.\n",
    "    - yhat = 1 / (1 + exp(-(X * Beta))) , that is how a predicted value is squished to 0-1 range using sigmoid function. The output is interpreted as a probability from a Binomial probability distribution function for the class labeled 1, if the two classes in the problem are labeled 0 and 1.\n",
    "    - Application: Email spam or not\n",
    "    \n",
    "\n",
    "- Equation\n",
    "    - p(x|beta) = P(y=1| x,beta) = 1-P(y=0|x,beta); beta is the parameters, and x is the independent features. Given x and beta what is probability of class 1.\n",
    "    - p(x|beta) = g(b0 + b1x1 + b2x2 + .. + bnxn) : Probability of class 1\n",
    "    - We use training date to estimate the betas. Say m data points, n features : n+1 betas\n",
    "    - We make use of MLE - Maximum likelyhood estimation to get the betas. We use betas, x and y to get a function to calculate the likelihood of getting the observed class, and then we use betas to maximize the likelihood\n",
    "    - What it the formulae? PRODUCT(i=1 to m){P(x(i))** y(i) X (1-P(x(i))** (1-y(i))}; change it to log likelihood and then choose betas to maximize it \n",
    "    - m is the number of data points; P(x(i)) = p(y(i)=1| x(i), beta); p(y(i)=0| x(i), beta) = 1 - P(x(i))\n",
    "    - x(i) is a data-point/record with n-features\n",
    "\n",
    "\n",
    "- How to maximize the LogLikelihood function for beta?\n",
    "    - We change it to -ve log loss function to be minimized, and use gradient descent to get the betas\n",
    "    - A gradient for a parameter is the partial derivative of the loss function w.r.t. that parameter\n",
    "\n",
    "    \n",
    "- Complexity:\n",
    "    - O(I *M *N) Time\n",
    "    - O(N) Space\n",
    "    \n",
    "- Prep:\n",
    "    - Probability vs. Likelihood\n",
    "    - Meaning and generation of coefficients in case of continuos vs. discrete variables in Logistic regression\n",
    "    - Changing the y-axis from 0-1 to Log(odds) graph for continuos and discrete variables and forming the equations\n",
    "    - Log odds to sigmoid, to Likelihood calculation from sigmoid curve, and rotating the log-odds line to fins the best fitting lines using MLE\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Main function for logistic regression\n",
    "##\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "def logistic_regression(x, y, iterations=100, learning_rate=0.01):\n",
    "    ## Number of records and # of features\n",
    "    m = len(x); n=len(x[0])\n",
    "    beta_0, beta_other = initialize_params(n)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        gradient_beta_0, gradient_beta_other = compute_gradients(x, y, beta_0, beta_other, m, n)\n",
    "        beta_0, beta_other = update_params(gradient_beta_0, gradient_beta_other)\n",
    "        \n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the parameters \n",
    "##\n",
    "def initialize_params(n_features):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for _ in range(n_features)] \n",
    "    return beta_0, beta_other\n",
    "\n",
    "\n",
    "### Compute the gradients\n",
    "##\n",
    "def compute_gradients(x, y, beta_0, beta_other, m, n):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_0 = [0]*n\n",
    "    \n",
    "    for i, point in enumerate(x):   #iterate through each record in the data and calculate predicted value using the params\n",
    "        pred = logistic_function(point, beta_0, beta_other, n)\n",
    "        error = pred-y[i]\n",
    "        \n",
    "        for j, feature in enumerate(point):  #iterate through each feature for each record and compute the gradient\n",
    "            gradient_beta_0 += error/m\n",
    "            gradient_beta_other[j] += error*feature/m\n",
    "            \n",
    "    return gradient_beta_0, gradient_beta_other\n",
    "\n",
    "### Update the parameters\n",
    "##\n",
    "def update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate):\n",
    "    beta_0 -= gradient_beta_0*learning_rate\n",
    "    for _ in range(len(beta_other)):\n",
    "        beta_other[_] -=  gradient_beta_other[_]*learning_rate\n",
    "        \n",
    "    return beta_0, beta_other\n",
    "\n",
    "### Logistic Function\n",
    "##\n",
    "def logistic_function(point, beta_0, beta_other):\n",
    "    log_odds = sum(beta_other[i]*point[i]   for i in range(n)) + beta_other\n",
    "    prob = 1/(1+math.exp(-log_odds))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The above solution will become slow for a large dataset\n",
    "- Better approach is to use Mini-batch gradient descent\n",
    "    - Earlier we looped through the entire dataset to move step towards the target, this can become very inefficient in case of large datasets\n",
    "- It takes a random mini-batch from entire data set and calculates the gradient, this leads to faster computation and as the data is now smaller we're able to fit this into memory. \n",
    "- The downside is that the gradient is noisier. It may lead the gradient in the wrong directions. But, the parameters will be optimized over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_minibatch(x, y, beta_0, beta_other, m, n, batch_size):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0]*n\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        i = random.randint(0, m-1)\n",
    "        point = x[i]\n",
    "        pred = logistic_regression(point, beta_0, beta_other)\n",
    "        error = pred - y[i]\n",
    "        \n",
    "        for j in range(n):\n",
    "            gradient_beta_0+=error/batch_size\n",
    "            gradient_beta_other[j] += error*x[i][j]/batch_size\n",
    "    \n",
    "    return gradient_beta_0, gradient_beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5. Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Decision tree for classification\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.root_dict = None\n",
    "        self.tree_dict = None\n",
    "\n",
    "    def split_dataset(self, X, y, feature_idx, threshold):\n",
    "        \"\"\"\n",
    "        Splits dataset X into two subsets, according to a given feature\n",
    "        and feature threshold.\n",
    "\n",
    "        Args:\n",
    "            X: 2D numpy array with data samples\n",
    "            y: 1D numpy array with labels\n",
    "            feature_idx: int, index of feature used for splitting the data\n",
    "            threshold: float, threshold used for splitting the data\n",
    "\n",
    "        Returns:\n",
    "            splits: dict containing the left and right subsets\n",
    "            and their labels\n",
    "        \"\"\"\n",
    "\n",
    "        left_idx = np.where(X[:, feature_idx] < threshold)\n",
    "        right_idx = np.where(X[:, feature_idx] >= threshold)\n",
    "\n",
    "        left_subset = X[left_idx]\n",
    "        y_left = y[left_idx]\n",
    "\n",
    "        right_subset = X[right_idx]\n",
    "        y_right = y[right_idx]\n",
    "\n",
    "        splits = {\n",
    "           'left': left_subset,\n",
    "           'y_left': y_left,\n",
    "           'right': right_subset,\n",
    "           'y_right': y_right,\n",
    "        }\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def gini_impurity(self, y_left, y_right, n_left, n_right):\n",
    "        \"\"\"\n",
    "        Computes Gini impurity of a split.\n",
    "\n",
    "        Args:\n",
    "            y_left, y_right: target values of samples in left/right subset\n",
    "            n_left, n_right: number of samples in left/right subset\n",
    "\n",
    "        Returns:\n",
    "            gini_left: float, Gini impurity of left subset\n",
    "            gini_right: gloat, Gini impurity of right subset\n",
    "        \"\"\"\n",
    "\n",
    "        n_total = n_left + n_left\n",
    "\n",
    "        score_left, score_right = 0, 0\n",
    "        gini_left, gini_right = 0, 0\n",
    "\n",
    "        if n_left != 0:\n",
    "            for c in range(self.n_classes):\n",
    "                # For each class c, compute fraction of samples with class c\n",
    "                p_left = len(np.where(y_left == c)[0]) / n_left\n",
    "                score_left += p_left * p_left\n",
    "            gini_left = 1 - score_left\n",
    "\n",
    "        if n_right != 0:\n",
    "            for c in range(self.n_classes):\n",
    "                p_right = len(np.where(y_right == c)[0]) / n_right\n",
    "                score_right += p_right * p_right\n",
    "            gini_right = 1 - score_right\n",
    "\n",
    "        return gini_left, gini_right\n",
    "\n",
    "    def get_cost(self, splits):\n",
    "        \"\"\"\n",
    "        Computes cost of a split given the Gini impurity of\n",
    "        the left and right subset and the sizes of the subsets\n",
    "        \n",
    "        Args:\n",
    "            splits: dict, containing params of current split\n",
    "        \"\"\"\n",
    "        y_left = splits['y_left']\n",
    "        y_right = splits['y_right']\n",
    "\n",
    "        n_left = len(y_left)\n",
    "        n_right = len(y_right)\n",
    "        n_total = n_left + n_right\n",
    "\n",
    "        gini_left, gini_right = self.gini_impurity(y_left, y_right, n_left, n_right)\n",
    "        cost = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Finds the best feature and feature index to split dataset X into\n",
    "        two groups. Checks every value of every attribute as a candidate\n",
    "        split.\n",
    "\n",
    "        Args:\n",
    "            X: 2D numpy array with data samples\n",
    "            y: 1D numpy array with labels\n",
    "\n",
    "        Returns:\n",
    "            best_split_params: dict, containing parameters of the best split\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        best_feature_idx, best_threshold, best_cost, best_splits = np.inf, np.inf, np.inf, None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            for i in range(n_samples):\n",
    "                current_sample = X[i]\n",
    "                threshold = current_sample[feature_idx]\n",
    "                splits = self.split_dataset(X, y, feature_idx, threshold)\n",
    "                cost = self.get_cost(splits)\n",
    "\n",
    "                if cost < best_cost:\n",
    "                    best_feature_idx = feature_idx\n",
    "                    best_threshold = threshold\n",
    "                    best_cost = cost\n",
    "                    best_splits = splits\n",
    "\n",
    "        best_split_params = {\n",
    "            'feature_idx': best_feature_idx,\n",
    "            'threshold': best_threshold,\n",
    "            'cost': best_cost,\n",
    "            'left': best_splits['left'],\n",
    "            'y_left': best_splits['y_left'],\n",
    "            'right': best_splits['right'],\n",
    "            'y_right': best_splits['y_right'],\n",
    "        }\n",
    "\n",
    "        return best_split_params\n",
    "\n",
    "\n",
    "    def build_tree(self, node_dict, depth, max_depth, min_samples):\n",
    "        \"\"\"\n",
    "        Builds the decision tree in a recursive fashion.\n",
    "\n",
    "        Args:\n",
    "            node_dict: dict, representing the current node\n",
    "            depth: int, depth of current node in the tree\n",
    "            max_depth: int, maximum allowed tree depth\n",
    "            min_samples: int, minimum number of samples needed to split a node further\n",
    "\n",
    "        Returns:\n",
    "            node_dict: dict, representing the full subtree originating from current node\n",
    "        \"\"\"\n",
    "        left_samples = node_dict['left']\n",
    "        right_samples = node_dict['right']\n",
    "        y_left_samples = node_dict['y_left']\n",
    "        y_right_samples = node_dict['y_right']\n",
    "\n",
    "        if len(y_left_samples) == 0 or len(y_right_samples) == 0:\n",
    "            node_dict[\"left_child\"] = node_dict[\"right_child\"] = self.create_terminal_node(np.append(y_left_samples, y_right_samples))\n",
    "            return None\n",
    "\n",
    "        if depth >= max_depth:\n",
    "            node_dict[\"left_child\"] = self.create_terminal_node(y_left_samples)\n",
    "            node_dict[\"right_child\"] = self.create_terminal_node(y_right_samples)\n",
    "            return None\n",
    "\n",
    "        if len(right_samples) < min_samples:\n",
    "            node_dict[\"right_child\"] = self.create_terminal_node(y_right_samples)\n",
    "        else:\n",
    "            node_dict[\"right_child\"] = self.find_best_split(right_samples, y_right_samples)\n",
    "            self.build_tree(node_dict[\"right_child\"], depth+1, max_depth, min_samples)\n",
    "\n",
    "        if len(left_samples) < min_samples:\n",
    "            node_dict[\"left_child\"] = self.create_terminal_node(y_left_samples)\n",
    "        else:\n",
    "            node_dict[\"left_child\"] = self.find_best_split(left_samples, y_left_samples)\n",
    "            self.build_tree(node_dict[\"left_child\"], depth+1, max_depth, min_samples)\n",
    "\n",
    "        return node_dict\n",
    "\n",
    "    def create_terminal_node(self, y):\n",
    "        \"\"\"\n",
    "        Creates a terminal node.\n",
    "        Given a set of labels the most common label is computed and\n",
    "        set as the classification value of the node.\n",
    "\n",
    "        Args:\n",
    "            y: 1D numpy array with labels\n",
    "        Returns:\n",
    "            classification: int, predicted class\n",
    "        \"\"\"\n",
    "        classification = max(set(y), key=list(y).count)\n",
    "        return classification\n",
    "\n",
    "    def train(self, X, y, max_depth, min_samples):\n",
    "        \"\"\"\n",
    "        Fits decision tree on a given dataset.\n",
    "\n",
    "        Args:\n",
    "            X: 2D numpy array with data samples\n",
    "            y: 1D numpy array with labels\n",
    "            max_depth: int, maximum allowed tree depth\n",
    "            min_samples: int, minimum number of samples needed to split a node further\n",
    "        \"\"\"\n",
    "        self.n_classes = len(set(y))\n",
    "        self.root_dict = self.find_best_split(X, y)\n",
    "        self.tree_dict = self.build_tree(self.root_dict, 1, max_depth, min_samples)\n",
    "\n",
    "    def predict(self, X, node):\n",
    "        \"\"\"\n",
    "        Predicts the class for a given input example X.\n",
    "\n",
    "        Args:\n",
    "            X: 1D numpy array, input example\n",
    "            node: dict, representing trained decision tree\n",
    "\n",
    "        Returns:\n",
    "            prediction: int, predicted class\n",
    "        \"\"\"\n",
    "        feature_idx = node['feature_idx']\n",
    "        threshold = node['threshold']\n",
    "\n",
    "        if X[feature_idx] < threshold:\n",
    "            if isinstance(node['left_child'], (int, np.integer)):\n",
    "                return node['left_child']\n",
    "            else:\n",
    "                prediction = self.predict(X, node['left_child'])\n",
    "        elif X[feature_idx] >= threshold:\n",
    "            if isinstance(node['right_child'], (int, np.integer)):\n",
    "                return node['right_child']\n",
    "            else:\n",
    "                prediction = self.predict(X, node['right_child'])\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 6. DNN\n",
    "\n",
    "- PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(1, 4)\n",
      "(4, 1)\n",
      "(1, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "### BASICS\n",
    "\n",
    "print((np.array([0,0,1,0])).shape)  #vector\n",
    "print((np.array([[0,0,1,0]])).shape) #matrix\n",
    "print((np.array([[0],[0],[1],[0]])).shape)\n",
    "print((np.array([[[0],[0],[1],[0]]])).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs for training: [[5.54116778e-03]\n",
      " [4.52027080e-03]\n",
      " [9.93599779e-01]\n",
      " [1.62978367e-07]]\n"
     ]
    }
   ],
   "source": [
    "### PERCEPTRON CODE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "\n",
    "training_inputs = np.array([[0,0,1],    ##Training is a 4x3 matrix, 4-records, 3-features\n",
    "                            [1,1,1],\n",
    "                            [1,0,1],\n",
    "                            [0,1,1]])\n",
    "\n",
    "training_outputs = np.array([[0,0,1,0]]).T  ## Transposed it to make it a 4*1 matrix. What's the other way to do it?\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "## random.random() gnerates a number between 0 and 1. specifying a shape with in this function, you can generate a matrix of normally dist.\n",
    "## number between 0 and 1\n",
    "\n",
    "### Random values b/w -1 to 1 with a mean of 0, normally distributed weights\n",
    "synaptic_weights = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "\n",
    "## Adjustment  = input*error*sigmoidderivative\n",
    "\n",
    "### EXPLAIN the calculation below\n",
    "\n",
    "for iteration in range(50000):\n",
    "    input_layer = training_inputs \n",
    "    output = sigmoid(np.dot(input_layer, synaptic_weights))\n",
    "    \n",
    "    ### Calculate the error, and take the gradient, and update the weights\n",
    "    error = training_outputs - output\n",
    "    adjustments = error*sigmoid_derivative(output)\n",
    "    synaptic_weights += np.dot(input_layer.T, adjustments)\n",
    "    \n",
    "print('Outputs for training: {}'.format(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81779661],\n",
       "       [0.42362076],\n",
       "       [0.97693232]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.random.random((3,1))-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "- Usable Percepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Neural Net\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Here you can declare the variables that later can be used'''\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        self.synaptic_weights = 2*np.random.random((3,1)) - 1\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x*(1 - x)   \n",
    "        \n",
    "    def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \n",
    "        for iterations in range(training_iterations):\n",
    "            output = self.think(training_inputs)\n",
    "            error = training_outputs - output\n",
    "            \n",
    "            adjustments = np.dot(training_inputs.T, error*self.sigmoid_derivative(output))\n",
    "            self.synaptic_weights += adjustments\n",
    "    \n",
    "    def think(self,inputs):\n",
    "        inputs = inputs.astype(float)\n",
    "        return self.sigmoid(np.dot(inputs, self.synaptic_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weights [[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "\n",
      " output: [[1.79815368e-02]\n",
      " [1.46383347e-02]\n",
      " [9.79216854e-01]\n",
      " [5.77341890e-06]]\n"
     ]
    }
   ],
   "source": [
    "## Making it a usable command-line function\n",
    "if __name__ == \"__main__\":\n",
    "    dnn = NeuralNetwork()\n",
    "    print('Random weights {}'.format(dnn.synaptic_weights))\n",
    "    \n",
    "    training_inputs = np.array([[0,0,1],    ##Training is a 4x3 matrix, 4-records, 3-features\n",
    "                            [1,1,1],\n",
    "                            [1,0,1],\n",
    "                            [0,1,1]])\n",
    "    training_outputs = np.array([[0,0,1,0]]).T\n",
    "    \n",
    "    dnn.train(training_inputs, training_outputs, 5000)\n",
    "    print('\\n output: {}'.format(dnn.think(training_inputs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "####  Neural Net\n",
    "- We want the numpy arrays for multiplication and addition operation, which is not the same or present in lists\n",
    "- For weights we will use random values which will be float so it's important we have inputs as float as well\n",
    "- We don't want weights or inputs to be too big as it may lead to gradient explosion, usually we keep it between [-1, 1]. Scale the input if needed\n",
    "- Biases are usually initialized to 0. Network may become dead if the neurons do not become active in the first iteration itself, as you will be just propagating 0's forward and backward.\n",
    "\n",
    "- The way we define the weights and biases in the __init__() function makes it easier for calculation as now we don't need to do a transpose everytime we do a multiplication between weights and inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- BP is basically nudging the weights to reduce the Cost function.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Questions\n",
    "- Why are we taking -ve of gradient? Gradient tells us the direction of the steepest ascent.\n",
    "- How many gradients do we take?\n",
    "- How do things change from 1 perceptron to a network? Going from a single loss to average loss for all inputs and then calculating the gradient, to calculating gradients for all the weights in the network\n",
    "- How you can you tell which changes to weights matter the most/least? Looking at the gradient! It tells you how sensitive is the cost function to each weight and bias\n",
    "- Neurons that fire together, wire together; intuition behind BP. Take a training example, and calculate what changes should be applied to the previous layers to make predicted value= actual value, and keep propagating it backwards. Now you know what changes need to be made in 1000's of weights and biases for a single training example to reduce the loss. Now do it for all the training examples and take the average, and make those changes in weights and biases.\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Sigmoid Vs. ReLu\n",
    "\n",
    "- Sigmoid has a vanishing gradient issue\n",
    "- ReLu is fast due to its super simple calculation\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "##### Softmax for multi-class classification\n",
    "- Generalization of logistic function to multiple dimensions, to predict multinomial probability distribution.\n",
    "- Softmax is exponential and enlarges differences - push one result closer to 1 while another closer to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51993858]\n",
      " [0.5201816 ]\n",
      " [0.52032996]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Shape 3,4\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):  ##Anything passed while creating the class object is called here\n",
    "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)  ## Gaussian/standard normal distribution and then scaling them\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "     \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = self.sigmoid(np.dot(inputs, self.weights) + self.biases)\n",
    "        # return self.output\n",
    "        \n",
    "        \n",
    "class Activation_Relu:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        # return self.output \n",
    "        \n",
    "## Layer-1 will always have input = # of features in the input data\n",
    "layer_1 = Layer_Dense(4, 5)\n",
    "layer_2 = Layer_Dense(5, 10) ## The consequent layer will always have input = # of neurons in the previous layer, and the # of neurons\n",
    "                            ### for this layer can be any arbitrary number\n",
    "    \n",
    "layer_1.forward(X)\n",
    "# print(layer_1.output)\n",
    "layer_2.forward(layer_1.output)\n",
    "# print(layer_2.output)\n",
    "\n",
    "layer_3 = Layer_Dense(10, 1)\n",
    "layer_3.forward(layer_2.output)\n",
    "print(layer_3.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
